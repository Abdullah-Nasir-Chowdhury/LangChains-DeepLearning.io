{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,openai\n",
    "from dotenv import main\n",
    "main.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_completion(prompt,temperature=0,max_tokens=100,model='gpt-3.5-turbo'):\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "    response=openai.ChatCompletion.create(\n",
    "        model=model,messages=messages,temperature=temperature,max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].messages['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model ='gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory=ConversationBufferMemory()\n",
    "conversation=ConversationChain(\n",
    "    llm=llm, memory=memory, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I am Abdullah.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Abdullah! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, I am Abdullah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I am Abdullah.\n",
      "AI: Hello Abdullah! How can I assist you today?\n",
      "Human: Where are you located?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I am an AI, so I don't have a physical location. I exist in the digital realm and can be accessed from anywhere with an internet connection.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Where are you located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I am Abdullah.\n",
      "AI: Hello Abdullah! How can I assist you today?\n",
      "Human: Where are you located?\n",
      "AI: I am an AI, so I don't have a physical location. I exist in the digital realm and can be accessed from anywhere with an internet connection.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Abdullah.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, I am Abdullah.\n",
      "AI: Hello Abdullah! How can I assist you today?\n",
      "Human: Where are you located?\n",
      "AI: I am an AI, so I don't have a physical location. I exist in the digital realm and can be accessed from anywhere with an internet connection.\n",
      "Human: What is my name?\n",
      "AI: Your name is Abdullah.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ConversationBufferMemory.load_memory_variables of ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi, I am Abdullah.'), AIMessage(content='Hello Abdullah! How can I assist you today?'), HumanMessage(content='Where are you located?'), AIMessage(content=\"I am an AI, so I don't have a physical location. I exist in the digital realm and can be accessed from anywhere with an internet connection.\"), HumanMessage(content='What is my name?'), AIMessage(content='Your name is Abdullah.')]))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, I am Abdullah.\\nAI: Hello Abdullah! How can I assist you today?\\nHuman: Where are you located?\\nAI: I am an AI, so I don't have a physical location. I exist in the digital realm and can be accessed from anywhere with an internet connection.\\nHuman: What is my name?\\nAI: Your name is Abdullah.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up!\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\":\"What's up!\"})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hey, the sun has risen, wake up!\\nAI: I am always awake...'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.load_memory_variables({})\n",
    "memory.save_context({\"input\":\"Hey, the sun has risen, wake up!\"},\n",
    "                     {\"output\":\"I am always awake...\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nAI: Cool'}\n"
     ]
    }
   ],
   "source": [
    "memory=ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\":\"Hi\"},{\"output\":\"What's up!\"})\n",
    "memory.save_context({\"input\":\"Not much, just hanging\"}, {\"output\":\"Cool\"})\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-5BrO6nAzjMPI4FIfhxA3i7yU on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "conversation=ConversationChain(\n",
    "    llm=llm, memory=memory, verbose=False\n",
    ")\n",
    "conversation.predict(input=\"Hi, I am Abdullah\")\n",
    "conversation.predict(input=\"The sun is red\")\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tiktoken) (2022.3.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationTokenBufferMemory(llm=llm,max_token_limit=50)\n",
    "memory.save_context({\"input\":\"AI is what?\"},{\"output\":\"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},{\"output\":\"Beautiful\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks what is on schedule for the day. The AI responds with a detailed schedule, including a meeting with the product team, working on the LangChain project, and a lunch meeting with a customer to discuss the latest in AI. The AI also mentions the need to bring a laptop to show the latest LLM demo during the lunch meeting.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\":\"Hello\"},{\"output\":\"What's up\"})\n",
    "memory.save_context({\"input\":\"What is on schedule today?\"}, {\"output\":f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
